{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'html5lib.treebuilders' has no attribute '_base'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-30946feeb726>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdammit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnicodeDammit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m from .element import (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[0mregister_treebuilders_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_htmlparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_html5lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[0mregister_treebuilders_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_html5lib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_html5lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTreeBuilderForHtml5lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml5lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtreebuilders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeBuilder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamespaceHTMLElements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'html5lib.treebuilders' has no attribute '_base'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import date\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: html5lib in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from html5lib) (1.15.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\admin\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scraper:\n",
    "    today = re.sub(\"-\",\".\",date.today().isoformat())\n",
    "    def __init__(self, title, path, start_date= today, end_date= today):\n",
    "        self.title = title\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.path = path\n",
    "\n",
    "        self.__start_page = 1\n",
    "        self.__MAX_NEWS = 10\n",
    "        self.__COLUMNS = [\"title\", \"date\", \"press\", \"contents\"] \n",
    "        self.__DATA = []\n",
    "\n",
    "    def naver_news_scraper(self):\n",
    "        while True:\n",
    "            # 네이버 뉴스페이지 스크레핑\n",
    "            url = \"https://search.naver.com/search.naver?where=news&query={title}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={start_date}&de={end_date}&docid=&nso=so%3Ar%2Cp%3Afrom{start_date_except_dot}to{end_date_except_dot}%2Ca%3Aall&mynews=0&refresh_start={start_page}&related=0\".format(title= self.title, start_date= self.start_date, end_date= self.end_date, start_date_except_dot= re.sub(\".\", \"\", self.start_date), end_date_except_dot= re.sub(\".\", \"\", self.end_date), start_page= self.__start_page)\n",
    "            request = requests.get(url)\n",
    "            html = BeautifulSoup(request.text, \"html.parser\")\n",
    "\n",
    "            # 페이지에 있는 뉴스기사 개수\n",
    "            amount_news = html.find_all(class_ = \"news_area\")\n",
    "\n",
    "            for an in amount_news:\n",
    "                # 뉴스에 관한 정보\n",
    "                Press = []\n",
    "                URL = []\n",
    "                info = html.find(class_ = \"info_group\")\n",
    "                \n",
    "                # 언론사\n",
    "                for press in info.find_all(\"a\"):\n",
    "                    Press.append(press.text)\n",
    "                    URL.append(press[\"href\"])\n",
    "                NewsPress = Press[0]\n",
    "\n",
    "                # 날짜\n",
    "                SpanData = []\n",
    "                span = info.find_all(\"span\")\n",
    "                for sd in span:\n",
    "                    SpanData.append(sd.text)\n",
    "                NewsDate = SpanData[-1]\n",
    "                \n",
    "                print(Press)\n",
    "                print(URL)\n",
    "                # 네이버 뉴스 URL 추출\n",
    "                # if \"네이버뉴스\" in info:\n",
    "                #     each_url  = URL[-1]\n",
    "                #     self.each_news_scraper(each_url)\n",
    "\n",
    "                # self.__DATA.append([self.__newstitle, NewsDate, NewsPress, self.__newscontents])\n",
    "\n",
    "            if amount_news != self.__MAX_NEWS:\n",
    "                break\n",
    "            else:\n",
    "                self.__start_page += 1\n",
    "        \n",
    "        # 데이터 저장\n",
    "        self.save()\n",
    "\n",
    "    def each_news_scraper(self, url):\n",
    "        news = requests.get(url, headers={\"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"})\n",
    "        soup = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "        # 뉴스 제목\n",
    "        newstitle = soup.find(id = \"articleTitle\").text\n",
    "\n",
    "        # 뉴스 본문\n",
    "        newscontents = soup.find(id = \"articleBodyContents\").text\n",
    "\n",
    "        self.__newstitle = newstitle\n",
    "        self.__newscontents = newscontents\n",
    "\n",
    "    def save(self):\n",
    "        df = pd.DataFrame(data = self.__DATA, columns= self.__COLUMNS)\n",
    "        df.to_csv(self.path + \"{name} NaverNewsScap {start} to {end}.csv\".format(name = self.title, start = self.start_date, end = self.end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-38a1d077d667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0m__start_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 네이버 뉴스페이지 스크레핑\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://search.naver.com/search.naver?where=news&query={title}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={start_date}&de={end_date}&docid=&nso=so%3Ar%2Cp%3Afrom{start_date_except_dot}to{end_date_except_dot}%2Ca%3Aall&mynews=0&refresh_start={start_page}&related=0\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date_except_dot\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date_except_dot\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_page\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0m__start_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "title = \"여행\"\n",
    "start_date = \"2021.01.29\"\n",
    "end_date = \"2021.01.29\"\n",
    "__start_page = 1\n",
    "# 네이버 뉴스페이지 스크레핑\n",
    "url = \"https://search.naver.com/search.naver?where=news&query={title}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={start_date}&de={end_date}&docid=&nso=so%3Ar%2Cp%3Afrom{start_date_except_dot}to{end_date_except_dot}%2Ca%3Aall&mynews=0&refresh_start={start_page}&related=0\".format(title= title, start_date= start_date, end_date= end_date, start_date_except_dot= re.sub(\".\", \"\", start_date), end_date_except_dot= re.sub(\".\", \"\", end_date), start_page= __start_page)\n",
    "request = requests.get(url)\n",
    "html = BeautifulSoup(request.text, \"html.parser\")\n",
    "\n",
    "# 페이지에 있는 뉴스기사 개수\n",
    "amount_news = html.find_all(class_ = \"news_area\")\n",
    "\n",
    "for an in amount_news:\n",
    "    # 뉴스에 관한 정보\n",
    "    Press = []\n",
    "    URL = []\n",
    "    info = html.find(class_ = \"info_group\")\n",
    "\n",
    "    # 언론사\n",
    "    for press in info.find_all(\"a\"):\n",
    "        Press.append(press.text)\n",
    "        URL.append(press[\"href\"])\n",
    "        NewsPress = Press[0]\n",
    "\n",
    "    # 날짜\n",
    "    SpanData = []\n",
    "    span = info.find_all(\"span\")\n",
    "    for sd in span:\n",
    "        SpanData.append(sd.text)\n",
    "        NewsDate = SpanData[-1]\n",
    "\n",
    "    print(Press)\n",
    "    print(URL)\n",
    "    # 네이버 뉴스 URL 추출\n",
    "    # if \"네이버뉴스\" in info:\n",
    "    #     each_url  = URL[-1]\n",
    "    #     self.each_news_scraper(each_url)\n",
    "\n",
    "    # self.__DATA.append([self.__newstitle, NewsDate, NewsPress, self.__newscontents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}